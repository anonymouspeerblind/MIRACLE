# LLM as a judge
import transformers
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from tqdm import tqdm
import pandas as pd
import json
import time
import plotly.graph_objects as go
from huggingface_hub import login
from huggingface_hub import InferenceClient
login(token = "HF token")

def llm_judge():
    humans        = pd.read_csv("Path to surgeons annotation csv")
    human_remarks = dict(zip(humans["Record"], humans["Remarks"]))

    with open("path to remarks file generated by LLMs", "r") as js:
        llm_remarks = json.load(js)
    
    precontext  = "I used an LLM to generate remarks that states whether or not a patient, who is considered for a lung cancer surgery, will be having postoperative complications, by considering preoperative features. Along with that, I asked several practicing surgeons, to provide human remarks on whether or not the same patient will have postoperative complication, considering the same preoperative features. The remark generated by LLM is given as:\n"
    midcontext  = "The remark provided by human surgeon is given as:\n"
    postcontext = "Your task is to provide a 'total rating' scoring how well the LLM generated remarks align to that of human given remarks, given human given remarks are the ground truth. The main point of alignment, is the intensity of the risk of postoperative complication of the patient. Thus, you will be acting as judge between the two type of remarks. Give your answer as a 0, 1 or 2, where 0 means that the LLM generated remark is not aligning to human at all, 1 means that the LLM generated remarks are somewhat aligned to that of humans and 2 means that the LLM generated remarks are completely aligned to that of humans. Reason well before giving the answer, as we want the accuracy of scoring to be optimal. Provide a direct answer without any sentence to start answer. Give just the score without any salutation."

    model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = "")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        cache_dir = "",
        dtype="auto",
        device_map="auto",
        attn_implementation="flash_attention_2"
    )

    judgement = dict()
    for record_id in tqdm(human_remarks):
        surgeons = human_remarks[record_id]
        llm      = llm_remarks[str(float(record_id))]['remarks']

        text_prompt = precontext + llm + "\n" + midcontext + surgeons + "\n" + postcontext
        messages = [
        {"role": "system", "content": "You are an LLM that act as a judge and provide answer as per prompted, giving ratings for the comparison."},
        {"role": "user", "content": text_prompt},
        ]
        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to('cuda')

        terminators = [
            tokenizer.eos_token_id,
            tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        outputs = model.generate(
            input_ids,
            max_new_tokens=500
        )
        response = outputs[0][input_ids.shape[-1]:]
        idx_think = tokenizer.decode(response, skip_special_tokens=True).index("</think>")
        
        judgement[record_id] = int(tokenizer.decode(response, skip_special_tokens=True)[idx_think+8:].strip())
        with open("Path to save judgement file for csv", 'w') as js:
            json.dump(judgement, js, indent = 4)

def stats():
    with open("Path to save judgement file for csv LLM1", "r") as js:
        scores1 = json.load(js)
    scores1_dict = {"Completely unaligned": 0, "Somewhat aligned": 0, "Completely aligned": 0}
    for record in scores1:
        if scores1[record] == 0:
            scores1_dict["Completely unaligned"] += 1
        elif scores1[record] == 1:
            scores1_dict["Somewhat aligned"] += 1
        else:
            scores1_dict["Completely aligned"] += 1
    
    with open("Path to save judgement file for csv LLM2", "r") as js:
        scores2 = json.load(js)
    scores2_dict = {"Completely unaligned": 0, "Somewhat aligned": 0, "Completely aligned": 0}
    for record in scores2:
        if scores2[record] == 0:
            scores2_dict["Completely unaligned"] += 1
        elif scores2[record] == 1:
            scores2_dict["Somewhat aligned"] += 1
        else:
            scores2_dict["Completely aligned"] += 1

    with open("Path to save judgement file for csv LLM3", "r") as js:
        scores3 = json.load(js)
    scores3_dict = {"Completely unaligned": 0, "Somewhat aligned": 0, "Completely aligned": 0}
    for record in scores3:
        if scores3[record] == 0:
            scores3_dict["Completely unaligned"] += 1
        elif scores3[record] == 1:
            scores3_dict["Somewhat aligned"] += 1
        else:
            scores3_dict["Completely aligned"] += 1
    
    categories   = list(scores1_dict.keys())
    frequencies1 = list(scores1_dict.values())
    frequencies2 = list(scores2_dict.values())
    frequencies3 = list(scores3_dict.values())

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=categories,
        y=frequencies1,
        name="Deep Seek R1-distill Qwen-32B",
        text=frequencies1,
        textposition='outside'
    ))

    fig.add_trace(go.Bar(
        x=categories,
        y=frequencies2,
        name="Llama 3.3 70B-Instruct",
        text=frequencies2,
        textposition='outside'
    ))

    fig.add_trace(go.Bar(
        x=categories,
        y=frequencies3,
        name="OpenBioLLM-70B",
        text=frequencies3,
        textposition='outside'
    ))

    fig.update_layout(
        xaxis=dict(
            title=dict(
                text="Alignment",
                standoff=8
            )
        ),
        yaxis=dict(
            title=dict(
                text="Frequency",
                standoff=8 
            )
        ),
        barmode='group', 
        height=600, 
        width=800, 
        margin=dict(t=50, b=100, l=50, r=50),
        legend=dict(
            x=0,
            y=1,
            font=dict(size=10, color='black'),
            xanchor="left",
            yanchor="top",
            bgcolor="rgba(255,255,255,0.5)",
            bordercolor="black",           
            borderwidth=1
        )
        )

    fig.write_image("Path to save plot for judgement")

if __name__ == "__main__":
    llm_judge()
    # stats()
