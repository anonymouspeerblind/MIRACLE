# LLM as a judge
import transformers
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from tqdm import tqdm
import pandas as pd
import json
import time
from huggingface_hub import login
from huggingface_hub import InferenceClient
login(token = "")

if __name__ == "__main__":
    humans        = pd.read_csv("<path to human annotated test set>")
    human_remarks = dict(zip(humans["Record"], humans["Remarks"]))

    with open("<path to remarks file from LLM in test cohort>", "r") as js:
        llm_remarks = json.load(js)
    
    precontext  = "I used an LLM to generate remarks that states whether or not a patient, who is considered for a lung cancer surgery, will be having postoperative complications, by considering preoperative features. Along with that, I asked several practicing surgeons, to provide human remarks on whether or not the same patient will have postoperative complication, considering the same preoperative features. The remark generated by LLM is given as:\n"
    midcontext  = "The remark provided by human surgeon is given as:\n"
    postcontext = "Your task is to provide a 'total rating' scoring how well the LLM generated remarks align to that of human given remarks, given human given remarks are the ground truth. The main point of alignment, is the intensity of the risk of postoperative complication of the patient. Thus, you will be acting as judge between the two type of remarks. Give your answer as a 0, 1 or 2, where 0 means that the LLM generated remark is not aligning to human at all, 1 means that the LLM generated remarks are somewhat aligned to that of humans and 2 means that the LLM generated remarks are completely aligned to that of humans. Reason well before giving the answer, as we want the accuracy of scoring to be optimal. Provide a direct answer without any sentence to start answer. Give just the score without any salutation."

    model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = "")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        cache_dir = "",
        torch_dtype="auto",
        device_map="auto",
        attn_implementation="flash_attention_2"
    )

    judgement = dict()
    for record_id in tqdm(human_remarks):
        surgeons = human_remarks[record_id]
        llm      = llm_remarks[str(float(record_id))]['remarks']

        text_prompt = precontext + llm + "\n" + midcontext + surgeons + "\n" + postcontext
        messages = [
        {"role": "system", "content": "You are an LLM that act as a judge and provide answer as per prompted, giving ratings for the comparison."},
        {"role": "user", "content": text_prompt},
        ]
        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to('cuda')

        terminators = [
            tokenizer.eos_token_id,
            tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        outputs = model.generate(
            input_ids,
            max_new_tokens=2000
        )
        response = outputs[0][input_ids.shape[-1]:]
        idx_think = tokenizer.decode(response, skip_special_tokens=True).index("</think>")
        
        judgement[record_id] = int(tokenizer.decode(response, skip_special_tokens=True)[idx_think+8:].strip())
        with open("<path to save remarks file from LLM>", 'w') as js:
            json.dump(judgement, js, indent = 4)